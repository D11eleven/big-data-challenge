{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NotebookHW1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WosHdRWdmpw",
        "outputId": "1a38a4bf-5402-4ee8-accb-b7f097a23b89"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.3'\n",
        "spark_version = 'spark-3.2.0'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Fetched 252 kB in 2s (107 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xISV-9eR6Yah",
        "outputId": "a7f6d80c-757f-4ff5-bc7f-99b96a1a426c"
      },
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-05 04:41:40--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 914037 (893K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.9.jar.3’\n",
            "\n",
            "postgresql-42.2.9.j 100%[===================>] 892.61K  5.26MB/s    in 0.2s    \n",
            "\n",
            "2021-12-05 04:41:41 (5.26 MB/s) - ‘postgresql-42.2.9.jar.3’ saved [914037/914037]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEQFTWiV6g2h"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3WG0BBzzpkt"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qik6uGSV9BWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1976ca0d-3fa1-4c48-fc41-196e59f16817"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url=\"https://dshbigdatahw.s3.us-east-2.amazonaws.com/amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz\"\n",
        "# https://dshactivity3-2021.s3.us-east-2.amazonaws.com/user_data.csv\n",
        "spark.sparkContext.addFile(url)\n",
        "review_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Lawn_and_Garden_v1_00.tsv.gz\"), sep=\"\\t\", header=True, inferSchema=True)\n",
        "\n",
        "# Show DataFrame\n",
        "review_df.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|marketplace|customer_id|     review_id|product_id|product_parent|       product_title|product_category|star_rating|helpful_votes|total_votes|vine|verified_purchase|     review_headline|         review_body|review_date|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "|         US|   32787517| RED72VWWCOS7S|B008HDQYLQ|     348668413|Garden Weasel Gar...| Lawn and Garden|          1|            2|          8|   N|                Y|            One Star|I don't hate the ...| 2015-08-31|\n",
            "|         US|   16374060| RZHWQ208LTEPV|B005OBZBD6|     264704759|10 Foot Mc4 Solar...| Lawn and Garden|          5|            0|          0|   N|                Y|          Five Stars|        worked great| 2015-08-31|\n",
            "|         US|    9984817|R37LBC3XAVLYOO|B00RQL8U2G|      95173602|GE String A Long ...| Lawn and Garden|          5|            4|          5|   N|                Y|just what i neede...|just what i neede...| 2015-08-31|\n",
            "|         US|   12635190|R3L7XJMA0MVJWC|B0081SBO4Y|     835659279|Key Pair Lawn Wit...| Lawn and Garden|          5|            0|          0|   N|                Y|                Keys|Needed replacemen...| 2015-08-31|\n",
            "|         US|   43905102|R2I2GHSI7T1UBN|B008E6OK3U|     539243347|Zodiac R0502300 L...| Lawn and Garden|          1|            5|          6|   N|                Y|       Too expensive|Assuming you don'...| 2015-08-31|\n",
            "|         US|   52596997|R2GFFKHK4I6VMX|B00W6NTULY|     337446474|Hirts Gardens Swe...| Lawn and Garden|          5|            0|          0|   N|                Y|                Nice|Beautifully packa...| 2015-08-31|\n",
            "|         US|   43871104|R1R0UDX2XAN1S4|B00GXUMYKA|     468857193|AGPtEK 12 PCS Smo...| Lawn and Garden|          4|            0|          0|   N|                Y|These were pretty...|These were pretty...| 2015-08-31|\n",
            "|         US|   11346008|R22C8FMBSTFRY8|B005EIX8JS|     125753094|Design Toscano Ea...| Lawn and Garden|          5|            2|          2|   N|                Y|Kids love it. WIs...|Its in the center...| 2015-08-31|\n",
            "|         US|   49206471|R118NNIQ75XPGO|B000HJBKMQ|     834273114|TERRO T300 Liquid...| Lawn and Garden|          3|            0|          0|   N|                Y|      A little messy|The ants were att...| 2015-08-31|\n",
            "|         US|   37596267|R30HYXHZQ49621|B004LY59V6|     612086079|BLACK+DECKER LBXR...| Lawn and Garden|          2|            0|          0|   N|                Y|Does not hold a c...|This is advertise...| 2015-08-31|\n",
            "|         US|   31554283|R3EMLKY0GF1E90|B00CAVM85M|     280334010|Reach 'n Spray Pe...| Lawn and Garden|          5|            0|          0|   N|                Y|          Five Stars|Well made product...| 2015-08-31|\n",
            "|         US|   43211735|R23BX7EGJMGQJR|B00DP6X1LG|     233116679|Puro-Kleen Ultra-...| Lawn and Garden|          5|            1|          2|   N|                Y|It's easy to cut ...|I used this for a...| 2015-08-31|\n",
            "|         US|   25705116|R2Z4B6SDEAZF6E|B00025H2PY|     592807498|Diatomaceous Eart...| Lawn and Garden|          5|            0|          0|   N|                Y|          Five Stars|Great stuff. Gets...| 2015-08-31|\n",
            "|         US|   47041108|R35289PGJERP5J|B0079GHJXY|     408290044|Perky-Pet 312C Pa...| Lawn and Garden|          5|            0|          0|   N|                Y|          Five Stars|   Very good quality| 2015-08-31|\n",
            "|         US|    1534667|R39BPRMDKKIZL2|B004HFJ762|     404737140|Crossbow Dow Spec...| Lawn and Garden|          1|            4|          6|   N|                Y|Wrong Product- No...|This product was ...| 2015-08-31|\n",
            "|         US|   52287759| R6WFPPBS1DZMG|B00004RAGL|     773636542|Apex REM 15 15-Fo...| Lawn and Garden|          5|            0|          0|   N|                Y|dehumidifier drai...|the hose worked w...| 2015-08-31|\n",
            "|         US|   37010286| RK72M0ZBV9YLS|B010PWBNNK|     461072629|Elucto Electric B...| Lawn and Garden|          1|            3|          3|   N|                Y|   not easy it seems|I haven't killed ...| 2015-08-31|\n",
            "|         US|   30576559| RX5G150AUWRDJ|B00T77AWY6|     365662076|Ohuhu® 100 Ft Exp...| Lawn and Garden|          1|            0|          0|   N|                Y|          Five Stars|I m very disappoi...| 2015-08-31|\n",
            "|         US|   10291713|R1TMSZWIT21A31|B000UJH6HQ|     228393894|Toro 53746 Drip B...| Lawn and Garden|          3|            1|          2|   N|                Y|     Could be better|this is the fourt...| 2015-08-31|\n",
            "|         US|   50656780|R2FURVPW763CIM|B000HJBKMQ|     834273114|TERRO T300 Liquid...| Lawn and Garden|          5|            0|          0|   N|                Y|Sugar Ants are ho...|Best thing you ca...| 2015-08-31|\n",
            "+-----------+-----------+--------------+----------+--------------+--------------------+----------------+-----------+-------------+-----------+----+-----------------+--------------------+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPd0Hmj4fS22"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10qoqQv3_Y_w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827c6c45-53e4-43e0-d948-5dba2190f729"
      },
      "source": [
        "print(review_df.count())\n",
        "clean_review_df = review_df.dropna()\n",
        "print(clean_review_df.count())\n",
        "cleaner_review_df = clean_review_df.dropDuplicates()\n",
        "print(cleaner_review_df.count())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2557288\n",
            "2557005\n",
            "2557005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXNoC3ZJfUrq"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXtPi-Hw_dCe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719048e9-5ed3-482e-f20a-a959a3a928cf"
      },
      "source": [
        "cleaner_review_df.printSchema()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- marketplace: string (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_parent: integer (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            " |-- product_category: string (nullable = true)\n",
            " |-- star_rating: integer (nullable = true)\n",
            " |-- helpful_votes: integer (nullable = true)\n",
            " |-- total_votes: integer (nullable = true)\n",
            " |-- vine: string (nullable = true)\n",
            " |-- verified_purchase: string (nullable = true)\n",
            " |-- review_headline: string (nullable = true)\n",
            " |-- review_body: string (nullable = true)\n",
            " |-- review_date: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwHua-Lf8bs9"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpCse4fFCk9B"
      },
      "source": [
        "\n",
        "# from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
        "# # Convert String to Integer Type\n",
        "# df.withColumn(\"age\",df.age.cast(IntegerType()))\n",
        "# df.withColumn(\"age\",df.age.cast('int'))\n",
        "# df.withColumn(\"age\",df.age.cast('integer'))\n",
        "\n",
        "# # Using select\n",
        "# df.select(col(\"age\").cast('int').alias(\"age\"))\n",
        "\n",
        "# #Using selectExpr()\n",
        "# df.selectExpr(\"cast(age as int) age\")\n",
        "\n",
        "# #Using with spark.sql()\n",
        "# spark.sql(\"SELECT INT(age),BOOLEAN(isGraduated),DATE(jobStartDate) from CastExample\")\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18o3qtXMClbH"
      },
      "source": [
        "from pyspark.sql.types import IntegerType,BooleanType,DateType\n",
        "# cleaner_review_df = cleaner_review_df.withColumn(\"customer_id\",cleaner_review_df.customer_id.cast(IntegerType()))\n",
        "\n",
        "# cleaner_review_df = cleaner_review_df.withColumn(\"product_parent\",cleaner_review_df.product_parent.cast(IntegerType()))\n",
        "# cleaner_review_df = cleaner_review_df.withColumn(\"star_rating\",cleaner_review_df.star_rating.cast(IntegerType()))\n",
        "# cleaner_review_df = cleaner_review_df.withColumn(\"helpful_votes\",cleaner_review_df.helpful_votes.cast(IntegerType()))\n",
        "# cleaner_review_df = cleaner_review_df.withColumn(\"total_votes\",cleaner_review_df.total_votes.cast(IntegerType()))\n",
        "\n",
        "cleaner_review_df = cleaner_review_df.withColumn(\"review_date\",cleaner_review_df.review_date.cast(DateType()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W14xkPK8Cmsi",
        "outputId": "3317e273-cf1f-48ba-95ef-ded2244d7b3b"
      },
      "source": [
        "cleaner_review_df.printSchema()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- marketplace: string (nullable = true)\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- review_id: string (nullable = true)\n",
            " |-- product_id: string (nullable = true)\n",
            " |-- product_parent: integer (nullable = true)\n",
            " |-- product_title: string (nullable = true)\n",
            " |-- product_category: string (nullable = true)\n",
            " |-- star_rating: integer (nullable = true)\n",
            " |-- helpful_votes: integer (nullable = true)\n",
            " |-- total_votes: integer (nullable = true)\n",
            " |-- vine: string (nullable = true)\n",
            " |-- verified_purchase: string (nullable = true)\n",
            " |-- review_headline: string (nullable = true)\n",
            " |-- review_body: string (nullable = true)\n",
            " |-- review_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmDIsdZIgFeV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d260ddff-6ed8-4e40-bf48-66bcfa57f316"
      },
      "source": [
        "review_id_df = cleaner_review_df.select([\"review_id\", \"customer_id\", \"product_id\", \"product_parent\", \"review_date\"])\n",
        "review_id_df.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+----------+--------------+-----------+\n",
            "|     review_id|customer_id|product_id|product_parent|review_date|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "|R1005P9J6VW5R3|   52047618|B004T34BA4|     767857880| 2013-08-22|\n",
            "|R100G8CW43Z88H|   44491134|B00198KP7K|     483989260| 2014-01-11|\n",
            "|R100GVZLWL8W5W|   27118518|B00021FLQK|      35303180| 2013-06-08|\n",
            "|R100X0TTGJ0DJO|   11396402|B00GSPHS1U|     884654147| 2014-08-29|\n",
            "|R10163CIAEURW2|   18683796|B001Q3M80U|     318115509| 2013-05-25|\n",
            "|R101FKJY32QC1H|   15669933|B006IE17IG|     182631652| 2013-01-01|\n",
            "|R101XR37UEXIQP|   33446506|B00008W71T|     489049465| 2004-04-06|\n",
            "|R1025TDS5CIURA|   53010138|B000CSS520|     465533388| 2007-05-30|\n",
            "|R1026XUCSMBLNC|   40950391|B0013AV9AG|     627907179| 2013-03-15|\n",
            "|R102R94KOSEH61|   45904960|B001E8NXG0|      59904235| 2013-01-09|\n",
            "|R103VFU1DTRU2F|   12435754|B00K2KQ2QO|     406215080| 2014-07-18|\n",
            "|R103WHKEGIHZ5T|   20916354|B002S42A3Q|     613355079| 2013-08-14|\n",
            "| R104WQ0IJLWPD|   35420011|B000CFOUEU|     661887017| 2006-07-18|\n",
            "|R104XSCHM7NF6C|   14036055|B001HB35JO|     650928972| 2015-05-30|\n",
            "|R1053WQGHQS0KQ|   20977856|B00H4D2U9K|     989109742| 2014-09-11|\n",
            "|R105BXSC4LHYJN|    9471984|B0012XWTGW|     950149733| 2013-07-04|\n",
            "|R105I9QSUWN4BS|   20615336|B00CAB5AAW|     926868939| 2015-05-15|\n",
            "|R1061TMISR0DIS|    5186398|B00P5DTFGW|     736423725| 2014-11-03|\n",
            "|R106CK1S9868OG|   12919744|B00025H2PY|     592807498| 2012-12-28|\n",
            "|R106M2R7WEVYCT|   51892033|B004REK370|     811740234| 2014-12-16|\n",
            "+--------------+-----------+----------+--------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eZHIsqEgGGd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76123610-924a-45e7-fdd3-e8a3fda46ef6"
      },
      "source": [
        "products_df = cleaner_review_df.select([\"product_id\", \"product_title\"])\n",
        "products_df.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|product_id|       product_title|\n",
            "+----------+--------------------+\n",
            "|B004T34BA4|Achla Designs Hea...|\n",
            "|B00198KP7K|Grill Daddy Repla...|\n",
            "|B00021FLQK|Mr. Bar-B-Q Plati...|\n",
            "|B00GSPHS1U|Intex Sand Filter...|\n",
            "|B001Q3M80U|Jack Post CG-12Z ...|\n",
            "|B006IE17IG|STC EGH520 Portab...|\n",
            "|B00008W71T|Weber Slide Aside...|\n",
            "|B000CSS520|Troy-Bilt Pony 6....|\n",
            "|B0013AV9AG|Black & Decker CV...|\n",
            "|B001E8NXG0|Achla Designs Din...|\n",
            "|B00K2KQ2QO|KINGLAKE 100PCS W...|\n",
            "|B002S42A3Q|Heavy Duty Replac...|\n",
            "|B000CFOUEU|Rubbermaid  Horiz...|\n",
            "|B001HB35JO|Songbird Essentia...|\n",
            "|B00H4D2U9K|Eley Wall Mount G...|\n",
            "|B0012XWTGW|Heavenly Blue Mor...|\n",
            "|B00CAB5AAW|Buddha of Compass...|\n",
            "|B00P5DTFGW|1/4 Lb (113 Grams...|\n",
            "|B00025H2PY|Diatomaceous Eart...|\n",
            "|B004REK370|Char-Broil Stainl...|\n",
            "+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGoXdTnbcodl",
        "outputId": "55806f04-3757-476d-9da0-b625e37b9537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "products_df.count()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2557005"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYuCYWuv_9ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5726acff-872d-4b2b-ec42-5fd9e8cdba5e"
      },
      "source": [
        "customers_df = cleaner_review_df.groupby([\"customer_id\"]).count()\n",
        "customers_df =customers_df.withColumnRenamed('count' , 'customer_count')\n",
        "# df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\n",
        "# df_bostonLegible = df_boston.withColumnRenamed(\"zn\", \"Zoning\")\n",
        "customers_df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+\n",
            "|customer_id|customer_count|\n",
            "+-----------+--------------+\n",
            "|   13507163|             1|\n",
            "|   42192220|             3|\n",
            "|   16022693|             2|\n",
            "|   17054071|             1|\n",
            "|   31950090|             3|\n",
            "|   23669123|             1|\n",
            "|   20901680|             1|\n",
            "|   51281657|             5|\n",
            "|   42050941|             2|\n",
            "|   47844876|             2|\n",
            "|   23745825|             1|\n",
            "|   51168318|             1|\n",
            "|   16181580|             2|\n",
            "|   50699952|            15|\n",
            "|   43483731|             8|\n",
            "|    2050707|             1|\n",
            "|   47884795|             1|\n",
            "|    4712293|             1|\n",
            "|   37267422|             2|\n",
            "|   11760241|             2|\n",
            "+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytC08J4igHEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3693634-ffa3-4e42-b037-e1fb50f634ef"
      },
      "source": [
        "vine_table_df = cleaner_review_df.select([\"review_id\", \"star_rating\", \"helpful_votes\", \"total_votes\", \"vine\"])\n",
        "vine_table_df.show()\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+-------------+-----------+----+\n",
            "|     review_id|star_rating|helpful_votes|total_votes|vine|\n",
            "+--------------+-----------+-------------+-----------+----+\n",
            "|R1005P9J6VW5R3|          2|            5|          5|   N|\n",
            "|R100G8CW43Z88H|          1|            1|          1|   N|\n",
            "|R100GVZLWL8W5W|          5|            0|          0|   N|\n",
            "|R100X0TTGJ0DJO|          5|            0|          0|   N|\n",
            "|R10163CIAEURW2|          5|            4|          4|   N|\n",
            "|R101FKJY32QC1H|          3|            0|          0|   N|\n",
            "|R101XR37UEXIQP|          5|           24|         24|   N|\n",
            "|R1025TDS5CIURA|          3|            4|          5|   N|\n",
            "|R1026XUCSMBLNC|          5|            0|          0|   N|\n",
            "|R102R94KOSEH61|          5|            1|          1|   N|\n",
            "|R103VFU1DTRU2F|          5|            0|          0|   N|\n",
            "|R103WHKEGIHZ5T|          5|            0|          0|   N|\n",
            "| R104WQ0IJLWPD|          4|            5|          7|   N|\n",
            "|R104XSCHM7NF6C|          2|            0|          0|   N|\n",
            "|R1053WQGHQS0KQ|          5|            1|          1|   N|\n",
            "|R105BXSC4LHYJN|          5|            1|          2|   N|\n",
            "|R105I9QSUWN4BS|          5|            0|          0|   N|\n",
            "|R1061TMISR0DIS|          5|            0|          0|   N|\n",
            "|R106CK1S9868OG|          1|            1|          4|   N|\n",
            "|R106M2R7WEVYCT|          3|            0|          0|   N|\n",
            "+--------------+-----------+-------------+-----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6vfEypcgHgK"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PZUA96qgIMt"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQgPpNnVnRwh"
      },
      "source": [
        "Postgres Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "985Vl1dtmpVU"
      },
      "source": [
        "# Configure settings for RDS\n",
        "# mode = \"append\"\n",
        "# jdbc_url=\"jdbc:postgresql://database-1.cl5tdbfnev78.us-east-2.rds.amazonaws.com:5432/<database-name>\"\n",
        "# config = {\"user\":\"postgres\", \n",
        "#           \"password\": \"postgres\", \n",
        "#           \"driver\":\"org.postgresql.Driver\"}\n",
        "\n",
        "\n",
        "#  Configure settings for RDS\n",
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://mypostgresdb.cl5tdbfnev78.us-east-2.rds.amazonaws.com:5432/HW\"\n",
        "config = {\"user\":\"root\", \n",
        "          \"password\": \"postgres\", \n",
        "          \"driver\":\"org.postgresql.Driver\"}\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9ErE-vyr41A"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BArWfJDxuKZ7"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yyZgvcUnaNN"
      },
      "source": [
        "# Write DataFrame to review_id_table in RDS\n",
        "\n",
        "review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rsTf2kmna3V"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NcPu5bWrekA"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q6Bm3kAncqx"
      },
      "source": [
        "# Write dataframe to customers table in RDS\n",
        "customers_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx3hMnlAD5FA"
      },
      "source": [
        "# Write dataframe to vine_table table in RDS\n",
        "vine_table_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9eUfH1q1O9Z",
        "outputId": "d13b7aff-e9b9-4bf2-a901-d2b4bf5ffd83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Write dataframe to products table in RDS\n",
        "products_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7b2d246aa999>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Write dataframe to products table in RDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproducts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjdbc_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'products'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.0-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.0-bin-hadoop2.7/python/lib/py4j-0.10.9.2-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.0-bin-hadoop2.7/python/lib/py4j-0.10.9.2-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o97.jdbc.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 121) (61c27bb699fa executor driver): java.sql.BatchUpdateException: Batch entry 31 INSERT INTO products (\"product_id\",\"product_title\") VALUES ('B004T34BA4','Achla Designs Heavy Duty Ultra Pole Shepherd''s Hook, 91-in Single (TSW-37)') was aborted: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B004T34BA4) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1357)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1382)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:494)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B004T34BA4) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2403)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2352)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1109)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1109)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2591)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2533)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2522)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:898)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2279)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:888)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:69)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.sql.BatchUpdateException: Batch entry 31 INSERT INTO products (\"product_id\",\"product_title\") VALUES ('B004T34BA4','Achla Designs Heavy Duty Ultra Pole Shepherd''s Hook, 91-in Single (TSW-37)') was aborted: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B004T34BA4) already exists.  Call getNextException to see other errors in the batch.\n\tat org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:153)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2242)\n\tat org.postgresql.core.v3.QueryExecutorImpl.flushIfDeadlockRisk(QueryExecutorImpl.java:1357)\n\tat org.postgresql.core.v3.QueryExecutorImpl.sendQuery(QueryExecutorImpl.java:1382)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:494)\n\tat org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:850)\n\tat org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:873)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1562)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:723)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"products_pkey\"\n  Detail: Key (product_id)=(B004T34BA4) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2505)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2241)\n\t... 20 more\n"
          ]
        }
      ]
    }
  ]
}